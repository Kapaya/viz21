<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <link href="https://fonts.googleapis.com/css?family=Merriweather:300,700" rel="stylesheet">
  <meta name="author" content="Kapaya Katongo, Geoffrey Litt and Daniel Jackson" />
  <title>Towards End-User Web Scraping for Customization</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <link rel="stylesheet" href="basic.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>

<header id="title-block-header">
<h1 class="title">Towards End-User Web Scraping for Customization</h1>
<p class="author">By Kapaya Katongo, Geoffrey Litt and Daniel Jackson</p>
<p class="date">This paper was accepted at the <a href="https://2021.programming-conference.org/home/px-2021">PX/21</a> workshop. Also available in a <a href="index.pdf">PDF version</a>.</p>
</header>
<p>Websites are malleable: users can run code in the browser to customize them. However, this malleability is typically only accessible to programmers with knowledge of HTML and Javascript. Previously, we developed a tool called Wildcard which empowers end-users to customize websites through a spreadsheet-like table interface without doing traditional programming. However, there is a limit to end-user agency with Wildcard, because programmers need to first create site-specific adapters mapping website data to the table interface. This means that end-users can only customize a website if a programmer has written an adapter for it, and cannot extend or repair existing adapters.</p>
<p>In this paper, we extend Wildcard with a new system for <em>end-user web scraping for customization</em>. It enables end-users to create, extend and repair adapters, by performing concrete demonstrations of how the website user interface maps to a data table. We describe three design principles that guided our system’s development and are applicable to other end-user web scraping and customization systems: (a) users should be able to scrape data and use it in a single, unified environment, (b) users should be able to extend and repair the programs that scrape data via demonstration and (c) users should receive live feedback during their demonstrations.</p>
<p>We have successfully used our system to create, extend and repair adapters by demonstration on a variety of websites and we provide example usage scenarios that showcase each of our design principles. Our ultimate goal is to empower end-users to customize websites in the course of their daily use in an intuitive and flexible way, and thus making the web more malleable for all of its users.</p>
<nav id="TOC" role="doc-toc">
<h2 id="toc-title">Contents</h2>
<ul>
<li><a href="#sec:introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="#sec:demos"><span class="toc-section-number">2</span> Motivating Examples</a></li>
<li><a href="#sec:implementation"><span class="toc-section-number">3</span> System Implementation</a></li>
<li><a href="#sec:design-principles"><span class="toc-section-number">4</span> Design Principles</a></li>
<li><a href="#sec:related-work"><span class="toc-section-number">5</span> Related Work</a></li>
<li><a href="#sec:conclusion"><span class="toc-section-number">6</span> Conclusion And Future Work</a></li>
<li><a href="#acknowledgments"><span class="toc-section-number">7</span> Acknowledgments</a></li>
<li><a href="#bibliography">References</a></li>
</ul>
</nav>
<h1 data-number="1" id="sec:introduction"><span class="header-section-number">1</span> Introduction</h1>
<p>Many websites on the internet do not meet the exact needs of all of their users. End-user web customization systems like Thresher <span class="citation" data-cites="hogue2005">[<a href="#ref-hogue2005" role="doc-biblioref">12</a>]</span>, Sifter <span class="citation" data-cites="huynh2006">[<a href="#ref-huynh2006" role="doc-biblioref">13</a>]</span> and Vegemite <span class="citation" data-cites="lin2009">[<a href="#ref-lin2009" role="doc-biblioref">19</a>]</span> help users to tweak and adapt websites to fit their unique requirements, ranging from reorganizing or annotating content on the website to automating common tasks. Millions of people also use tools like Greasemonkey <span class="citation" data-cites="zotero-90">[<a href="#ref-zotero-90" role="doc-biblioref">5</a>]</span> and Tampermonkey <span class="citation" data-cites="zotero-92">[<a href="#ref-zotero-92" role="doc-biblioref">6</a>]</span> to install browser userscripts, snippets of Javascript code which customize the behavior of websites.</p>
<p>In our prior work, we presented Wildcard <span class="citation" data-cites="litt2020a litt2020b">[<a href="#ref-litt2020a" role="doc-biblioref">20</a>,<a href="#ref-litt2020b" role="doc-biblioref">21</a>]</span>, a customization system which enables end-users to customize websites through direct manipulation. It does this by augmenting websites with a table view that shows their underlying structured data. The table is bidirectionally synchronized with the original website, so end-users can easily customize the website by interacting with the table, including sorting and filtering data, adding annotations, and running computations in a spreadsheet formula language. Wildcard enables end-users to be <em>creators</em> of browser userscripts (and not just consumers) without having to write Javascript code.</p>
<p>Wildcard has a key limitation. In order to enable end-users to customize a website, a programmer first needs to code a Javascript adapter that specifies how to scrape the website content and set up a bidirectional synchronization with Wildcard’s table view. Even though programmers can share adapters with end-users, this means that an end-user can only use Wildcard on websites where some programmer has already written an adapter. Additionally, if an adapter doesn’t scrape the desired data, or stops functioning correctly when a website changes, an end-user has no recourse to extend or repair it on their own.</p>
<div class="pdf-only">

</div>
<p>In this paper, we describe an addition to Wildcard: a system that enables end-users to create, extend and repair website adapters by demonstration within the browser. Using this scraping system, an end-user can perform web customizations using Wildcard on arbitrary websites, without ever needing to code an adapter. Through a series of examples, we show that our system can create Wildcard adapters on a variety of websites via demonstration (Section 2). We also describe key aspects of our system and how web scraping for customization leads to a constraint that simplifies the <em>wrapper induction</em> <span class="citation" data-cites="kushmerick2000">[<a href="#ref-kushmerick2000" role="doc-biblioref">16</a>]</span> task used to generalize user demonstrations (Section 3).</p>
<div class="html-only">
<figure>
<img src="media/overview.png" id="fig:overview" alt="Figure 1: Our work enables end-users to create Wildcard site adapters by demonstration." /><figcaption aria-hidden="true">Figure 1: Our work enables end-users to create Wildcard site adapters by demonstration.</figcaption>
</figure>
</div>
<p>We then describe the principles underlying the design of our system (Section 4):</p>
<ul>
<li><strong>Unified Environment</strong>: Users should be able to scrape data and interact with the scraped data in a single, unified environment. This minimizes the barrier to fluidly switching back and forth between the two tasks, rather than treating them as entirely independent tasks.</li>
<li><strong>Editing By Demonstration</strong>: Users should be able to not only create programs for scraping data by demonstration, but also extend and repair the programs by demonstration. This enables users to build on other users’ work, and is especially important in the context of web scraping since scrapers break as the underlying website changes.</li>
<li><strong>Live Programming</strong>: Users should receive live feedback as they perform demonstrations. The system should indicate how it is generalizing from the user’s example and what the resulting data will look like, so that the user can adjust their demonstrations on the fly and quickly arrive at the desired result.</li>
</ul>
<p>While each of these principles has been explored in prior work by various researchers, our contribution in this work is combining them in a novel way for the domain of end-user web scraping and customization.</p>
<p>Finally, we share our broader vision for <em>end-user web scraping for customization</em>, and some opportunities for future work, including a proposal for how Wildcard’s spreadsheet-like formula language might augment demonstrations to provide end-users with more expressiveness in the web scraping process (Section 6).</p>
<h1 data-number="2" id="sec:demos"><span class="header-section-number">2</span> Motivating Examples</h1>
<p>In this section, we show how end-users can create, extend and repair adapters for Wildcard via demonstration.</p>
<h2 data-number="2.1" id="creating-an-adapter"><span class="header-section-number">2.1</span> Creating An Adapter</h2>
<p>Jen wants to customize her experience on Weather.com by sorting the ten-day forecast based on the description of the weather on each day, allowing her to easily view all the sunny days. She starts the adapter creation process by clicking a context menu item within the Weather.com page, and hovers over a data value she might like to scrape.</p>
<video controls="controls" src="media/2.1.1.mp4" muted playsinline controls class>
</video>
<div class="pdf-only">

</div>
<div class="html-only">
<p>The system provides live feedback as Jen hovers, demonstrating the <strong>live programming</strong> principle:</p>
<ul>
<li>The row of data is annotated in the page with a border, to indicate that she will be demonstrating values from within that row.</li>
<li>The column of data is highlighted in the page with a green background, to show how the system has generalized her demonstration across all the rows in the data.</li>
<li>A table view appears at the bottom of the screen, and displays how the values will appear in the data table.</li>
</ul>
<p>Jen tries hovering over several other elements in the page, taking advantage of the live feedback environment to decide what data would be useful. After considering several options, she decides to save the date field in the first column of the table, and commits the action by clicking.</p>
</div>
<div class="pdf-only">

</div>
<video controls="controls" src="media/2.1.2.mp4" muted playsinline controls class>
</video>
<div class="html-only">
<p>Next, she performs a similar process to fill the next column with the weather descriptions. After filling both columns, she also tries hovering over previously scraped data, and the toolbar at the top of the page indicates which column corresponds to the previously scraped data. Finally, she ends the adapter creation process and is able to immediately sort the forecast by the weather description column, because Wildcard provides a <strong>unified environment</strong> that combines both scraping and customizing.</p>
</div>
<div class="pdf-only">

</div>
<video controls="controls" src="media/2.1.3.mp4" muted playsinline controls class>
</video>
<h2 data-number="2.2" id="extending-an-adapter"><span class="header-section-number">2.2</span> Extending An Adapter</h2>
<p>Jen has previously used Wildcard to customize timeanddate.com, sorting holidays by day of the week. She comes up with a new customization idea: sorting holidays by category so she can view all the federal holidays together. The current site adapter she is using does not populate the category column in the table, so she needs to extend the adapter. She can immediately perform the extension in the context of the page, using our system’s support for <strong>editing by demonstration</strong>.</p>
<div class="pdf-only">

</div>
<div class="html-only">
<p>While viewing the website, she clicks the “Edit Adapter” button above the Wildcard table to initiate the adapter editing process. As she hovers over the currently scraped values, the columns they belong to are highlighted. Finally, she clicks on “Federal Holiday” to add the new column of data and saves the changes. Jen then proceeds to sort the list by the type of holiday without the intervention of a programmer.</p>
</div>
<div class="pdf-only">

</div>
<video controls="controls" src="media/2.2.mp4" muted playsinline controls class>
</video>
<h2 data-number="2.3" id="repairing-an-adapter"><span class="header-section-number">2.3</span> Repairing An Adapter</h2>
<div class="html-only">
<p>Jen next visits Google Scholar to look up references for a project. Unfortunately, the customization she had applied to sort publications by their title (which is not natively supported by Google Scholar) is no longer working. In fact, the column in the Wildcard table that contained all the publication titles is empty, because the website’s internals changed and broke the adapter’s scraping logic. Jen can repair this on her own, again taking advantage of <strong>editing by demonstration</strong>.</p>
</div>
<div class="pdf-only">

</div>
<div class="html-only">
<p>Jen initiates the editing process, and initially hovers over the desired value to demonstrate the column she wants to scrape. However, the <strong>live programming</strong> interface indicates to her that the values would be populated into column D; instead, she wants the values to be inserted into column A where they previously appeared. So, Jen clicks on the symbol for column A to indicate that she wants to scrape the values into that column and demonstrates the first publication title. She then proceeds to re-apply her customization to the website by sorting the publications by their title.</p>
</div>
<div class="pdf-only">

</div>
<video controls="controls" src="media/2.3.mp4" muted playsinline controls class>
</video>
<div class="pdf-only">

</div>
<h1 data-number="3" id="sec:implementation"><span class="header-section-number">3</span> System Implementation</h1>
<p>We implemented our end-user web scraping system as an addition to the Wildcard browser extension. Prior to this work, website adapters were manually coded in Javascript by programmers. Now, adapters can be automatically created via demonstration. We start by describing our implementations of <em>wrapper induction</em> <span class="citation" data-cites="kushmerick2000">[<a href="#ref-kushmerick2000" role="doc-biblioref">16</a>]</span>, live programming, and editing by demonstration, and then discuss some of the current limitations of our system.</p>
<h2 data-number="3.1" id="wrapper-induction-algorithm"><span class="header-section-number">3.1</span> Wrapper Induction Algorithm</h2>
<p>In order to generate reusable scrapers from user demonstrations, our system solves the wrapper induction <span class="citation" data-cites="kushmerick2000">[<a href="#ref-kushmerick2000" role="doc-biblioref">16</a>]</span> task: generalizing from a small set of user-provided examples to a scraping specification that will work on other parts of the website, and on future versions of the website.</p>
<p>We take an approach similar to that used in other tools like Vegemite <span class="citation" data-cites="lin2009">[<a href="#ref-lin2009" role="doc-biblioref">19</a>]</span> and Sifter <span class="citation" data-cites="huynh2006">[<a href="#ref-huynh2006" role="doc-biblioref">13</a>]</span>:</p>
<ul>
<li>We generate a single <em>row selector</em> for the website: a CSS selector that returns a set of Document Object Model (DOM) elements corresponding to individual rows of the table.</li>
<li>For each column in the table, we generate a <em>column selector</em>, a CSS selector that returns the element containing the column value within that row.</li>
</ul>
<p>One important difference is that our algorithm only accepts row elements that have direct siblings with a similar structure. We refer to this as the <em>row-sibling</em> constraint. Later, we describe how the constraint provides a useful simplification of the wrapper induction task and discuss the resulting limitations this puts on our system.</p>
<p>When a user first demonstrates a column value, the generalization algorithm is responsible for turning the demonstration into a row selector that will correctly identify all the row elements in the website and a column selector that will correctly identify the element that contains the column value within a row element. During subsequent demonstrations, the generalization algorithm uses the generated row selector to find the row element that contains the column value and generates a column selector which identifies the corresponding column element.</p>
<p>At a high level, the wrapper induction algorithm’s challenge is to traverse far enough up in the DOM tree from the demonstrated element to find the element which corresponds to the row. We solve this using a heuristic; the basic intuition is to find a large set of elements with similar parallel structure. Consider the sample HTML layout in Figure 2, which displays a truncated table of superheroes, with each row containing some nested structure:</p>
<div class="pdf-only">

</div>
<div class="html-only">
<figure>
<img src="media/algorithm.png" id="fig:algorithm" alt="Figure 2: Our system applies a heuristic to identify DOM elements that correspond to rows in the data table." /><figcaption aria-hidden="true">Figure 2: Our system applies a heuristic to identify DOM elements that correspond to rows in the data table.</figcaption>
</figure>
</div>
<p>The user performs a demonstration by clicking on element <span class="math inline"><em>a</em></span> in Figure 2 containing “Tony Stark.” Our algorithm traverses upwards from the demonstrated element, considering each successive parent element (<span class="math inline"><em>b</em>1</span>, <span class="math inline"><em>c</em>1</span> and <span class="math inline"><em>d</em></span> in Figure 2) as a potential candidate for the row element. For each parent element <code>el</code>, the process is as follows:</p>
<ol type="1">
<li>compute a column selector <code>selector</code> that, when executed on <code>el</code>, only returns the demonstrated element</li>
<li>for each sibling <code>el'</code> of <code>el</code>, execute <code>selector</code> on <code>el'</code> and record whether the selector returns an element. If it does, this suggests that <code>el'</code> has some parallel structure to <code>el</code>.</li>
<li>compute <span class="math inline"><em>e</em><em>l</em><sub><em>s</em><em>i</em><em>b</em><em>l</em><em>i</em><em>n</em><em>g</em><em>s</em></sub></span>, the number of sibling elements of <code>el</code> which have parallel structure.</li>
</ol>
<p>Notice how the <em>row-sibling</em> constraint simplifies the problem. Row candidates without siblings with parallel structure (<span class="math inline"><em>b</em>1</span> in Figure 2) have <span class="math inline"><em>e</em><em>l</em><sub><em>s</em><em>i</em><em>b</em><em>l</em><em>i</em><em>n</em><em>g</em><em>s</em></sub></span> = 0, thus disqualifying them.</p>
<p>The algorithm stops traversing upwards once it reaches the <code>BODY</code> element. It chooses the element with the largest positive value of <span class="math inline"><em>e</em><em>l</em><sub><em>s</em><em>i</em><em>b</em><em>l</em><em>i</em><em>n</em><em>g</em><em>s</em></sub></span> as the row element, preferring nodes lower in the tree as a tiebreaker. It then generates a <em>row selector</em> which returns the row element and all its direct siblings. The final value of <code>selector</code> is the column selector since traverses from the row element to the demonstrated data value. These row and column selectors are then used to generate a scraping adapter which returns the DOM elements corresponding to a data row in the table and sets up the bidirectional synchronization.</p>
<h2 data-number="3.2" id="live-programming"><span class="header-section-number">3.2</span> Live Programming</h2>
<p>The idea of “liveness” in programming can be traced back to Tanimoto’s work on VIVA <span class="citation" data-cites="tanimoto1990">[<a href="#ref-tanimoto1990" role="doc-biblioref">26</a>]</span>. It generally describes programming environments in which programmers receive immediate feedback about a program while it is being created. In the context of Wildcard, adapters are the program, the table interface is the output of the program and the highlighting on the website is the visual representation of the program.</p>
<p>Live programming in our system is implemented by continually re-generating an adapter based on the DOM element under the user’s cursor and the previous demonstrations if any, reverting if the user hovers away and committing when the user clicks. The row and column selectors generated during the wrapper induction process are used to highlight all the matching elements on the website and create an adapter. Highlighting all the matching column elements on the website provides visual feedback about the system’s generalization to the user. Creating an adapter enables the system to populate the table view and set up the bidirectional synchronization. Because the table is populated and the bidirectional synchronization is set up, users can customize as they scrape. Live programming is possible in our system because the wrapper induction algorithm and adapters execute very quickly. We have yet to benchmark the performance in detail and compare it to other end-user web scraping systems like FlashProg <span class="citation" data-cites="mayer2015">[<a href="#ref-mayer2015" role="doc-biblioref">23</a>]</span> that offer live programming environments.</p>
<h2 data-number="3.3" id="editing-by-demonstration"><span class="header-section-number">3.3</span> Editing By Demonstration</h2>
<p>Our system generates adapters with the row selector and the column selectors used to scrape the data. The row selector is a CSS selector that identifies all the row elements of the data and the column selectors are CSS selectors that identify each column’s column elements.</p>
<p>When the editing process is initiated, the adapter’s row selector and column selectors are used to highlight the previously scraped values on the website. Furthermore, the generalization algorithm takes the adapter’s row selector and uses it as the basis to generate new column selectors after each demonstration. When a new column is demonstrated, our system appends the generated column selector to the list of column selectors. This is how adapters are extended to create new columns. When an existing column is demonstrated, our system replaces the column’s current column selector with the generated column selector. This is how adapters are repaired to fix broken columns.</p>
<p>Extending and repairing adapters in this manner is feasible because column selectors are independent of each other: changing one column’s selector does not affect another column’s selector. This is not the case for systems in which the output of demonstrations are dependent on each other. For example, in a web automation sequence that involves clicking on a button to open a menu and then entering text into the menu’s text input, the step that enters the text is not independent because it depends on the step that clicks the button to open the menu.</p>
<h2 data-number="3.4" id="limitations"><span class="header-section-number">3.4</span> Limitations</h2>
<p>Since our system is still under development, it has a variety of limitations. In this section we describe two of the most notable ones.</p>
<h3 data-number="3.4.1" id="wrapper-induction-algorithm-1"><span class="header-section-number">3.4.1</span> Wrapper Induction Algorithm</h3>
<p>The row-sibling constraint we mentioned earlier is important for the end goal of customization because row elements that are not direct siblings may not represent data on the website that should be related as part of the same table by customizations such as sorting and filtering. In Figure 3 we demonstrate two examples where this limitation becomes relevant.</p>
<div class="pdf-only">

</div>
<div class="html-only">
<figure>
<img src="media/limitations.png" id="fig:limitations" alt="Figure 3: Two example pages where our generalization algorithm does not currently work. The elements with the blue border correspond to rows of the data and the elements with green borders correspond to tables of data in each layout respectively. For the layout on the left, sorting could lead to rows from one table ending up in the other. For the layout on the right, sorting would lead to a distortion of the table since the column elements cannot be moved as a unit." /><figcaption aria-hidden="true">Figure 3: Two example pages where our generalization algorithm does not currently work. The elements with the blue border correspond to rows of the data and the elements with green borders correspond to tables of data in each layout respectively. For the layout on the left, sorting could lead to rows from one table ending up in the other. For the layout on the right, sorting would lead to a distortion of the table since the column elements cannot be moved as a unit.</figcaption>
</figure>
</div>
<p><em>Generalization Limitation 1</em> shows a case where the data is displayed in a grouped structure. Without the constraint that row elements have to be direct siblings, the row generalization algorithm could determine the row selector to be <em>.avenger</em> (elements with blue border) because it matches the largest number of parallel structures (has the largest <span class="math inline"><em>e</em><em>l</em><sub><em>s</em><em>i</em><em>b</em><em>l</em><em>i</em><em>n</em><em>g</em><em>s</em></sub></span>). While this may be the correct result for the task of extraction, it is not necessarily suitable for the task of customization. When the user sorts and filters, this could result in rows moving between the two tables, disrupting the nested layout and producing a confusing result. Because of this, our system currently does not support such layouts. In the future, we may explore the possibility of extracting multiple tables from a website and joining them together.</p>
<p><em>Generalization Limitation 2</em>, also in Figure 3, shows a case where the website contains one table of data in which rows are made up of alternating <code>H1</code> and <code>SPAN</code> tags (elements within blue border). This poses a challenge because each row does not correspond directly to a single DOM element; instead, each row consists of multiple consecutive DOM elements without any grouped structure. Moving the rows when customizing the website would require treating multiple consecutive elements as a single row. This is supported in the underlying Wildcard system, but not yet by our demonstration-based approach.</p>
<h3 data-number="3.4.2" id="data-loaded-after-initial-render"><span class="header-section-number">3.4.2</span> Data Loaded After Initial Render</h3>
<p>Our system currently does not support scraping data loaded after the initial website renders as the user scrolls. Site adapters hand-coded in Javascript can specify event listeners on the DOM to re-execute the scraping code when new data is loaded as a user scrolls. In future work, we plan to provide a mechanism for end-users to specify when a demonstrated adapter should re-execute its scraping code in response to user scrolling. We also do not support scraping data across multiple pages of related data, but this context poses more fundamental challenges to the idea of web customization, since users would somehow need to perform customizations across multiple pages in coordination.</p>
<h1 data-number="4" id="sec:design-principles"><span class="header-section-number">4</span> Design Principles</h1>
<p>Below, we describe our use of three existing design principles in a novel way for the domains of end-user web scraping and customization.</p>
<h2 data-number="4.1" id="unified-environment"><span class="header-section-number">4.1</span> Unified Environment</h2>
<p>In the previous iteration of Wildcard, web scraping was an entirely separate activity from customization. Programmers that wrote scraping adapters would need to switch into an IDE to write code as part of customizing a new website. This divide between tasks is common in other domains:</p>
<ul>
<li>In <strong>data science</strong>, workflows revolve between cleaning and using data but this often happens in different environments. The creators of Wrex <span class="citation" data-cites="drosos2020">[<a href="#ref-drosos2020" role="doc-biblioref">11</a>]</span>, an end-user programming-by-example system for data wrangling, reported that “although data scientists were aware of and appreciated the productivity benefits of existing data wrangling tools, having to leave their native notebook environment to perform wrangling limited the usefulness of these tools.” This was a major reason Wrex was developed as an add-on to Jupyter notebooks, the environment in which data scientists use their data.</li>
<li>In <strong>web scraping,</strong> if a user comes across an omission while working with data scraped from a website, they need to switch from the environment in which they are using the data to the environment in which they created their scraping code in order to edit and re-run it. This can be seen in many end-user web scraping systems like Rousillon <span class="citation" data-cites="chasins2018">[<a href="#ref-chasins2018" role="doc-biblioref">9</a>]</span>and FlashExtract <span class="citation" data-cites="le2014">[<a href="#ref-le2014" role="doc-biblioref">17</a>]</span> and commercial tools like import.io <span class="citation" data-cites="import.io">[<a href="#ref-import.io" role="doc-biblioref">14</a>]</span>, dexi.io <span class="citation" data-cites="dexi.io">[<a href="#ref-dexi.io" role="doc-biblioref">10</a>]</span>, Octoparse <span class="citation" data-cites="octoparse">[<a href="#ref-octoparse" role="doc-biblioref">24</a>]</span> and ParseHub <span class="citation" data-cites="parsehub">[<a href="#ref-parsehub" role="doc-biblioref">25</a>]</span>.</li>
<li>In <strong>web customization</strong>, the creators of Vegemite <span class="citation" data-cites="lin2009">[<a href="#ref-lin2009" role="doc-biblioref">19</a>]</span>, a system for end-user programming of mashups, reported that participants of its user study thought “it was confusing to use one technique to create the initial table, and another technique to add information to a new column.” This hints at the need for both a unified environment and a unified workflow.</li>
</ul>
<p>In this work, we have combined scraping and customization into a single, unified environment with a unified workflow. The goal is to minimize the environment switch between <em>extracting</em> the data and <em>using</em> the data. A user might start out by scraping some data on a website, and then switch to customizing the website using the results. Then, they might realize they need more data to perform their desired task, at which point they can easily extend the adapter by demonstrating new columns. All of these tasks take place right in the browser, where the user was initially already using the website. Instead of bringing the data to another tool, we have brought a tool to the data. This principle relates to the idea of “in-place toolchains” <span class="citation" data-cites="zotero-60">[<a href="#ref-zotero-60" role="doc-biblioref">4</a>]</span> for end-user programming systems: users should be able to program using familiar tools in the context where they already use their software.</p>
<p>Of course, there is value in specialized tools: Wildcard has nowhere near the full capabilities of spreadsheet software or databases. Nevertheless, we believe a single, unified environment for scraping and customization presents a significantly lower barrier to entry for customization.</p>
<h2 data-number="4.2" id="editing-by-demonstration-1"><span class="header-section-number">4.2</span> Editing By Demonstration</h2>
<p>Many end-user web scraping and macro systems allow users to <em>create</em> programs by demonstration but do not offer a way to <em>edit</em> them by demonstration. In Rousillon <span class="citation" data-cites="chasins2018">[<a href="#ref-chasins2018" role="doc-biblioref">9</a>]</span>, a web scraping program created by demonstration can only be edited through a high-level, block-based programming language called Helena <span class="citation" data-cites="zotero-51">[<a href="#ref-zotero-51" role="doc-biblioref">3</a>]</span>. Helena supports adding control flow logic (conditional execution, wait times etc) which is invaluable for automating access to websites. However, it does not support extending the web scraping code to add new columns after the demonstration or repairing it to provide new selectors if the website changes. In Vegemite <span class="citation" data-cites="lin2009">[<a href="#ref-lin2009" role="doc-biblioref">19</a>]</span>, a web automation program created through demonstration can only be edited by editing the text-based representation of the automation demonstrations. In fact, only the demonstrations used to perform automations on the scraped website data can be edited. If a user needs to add a new column or repair an existing one in the scraped data table, they need to re-demonstrate the columns and then re-run the automation script. One exception to existing editing models worth pointing out is import.io <span class="citation" data-cites="import.io">[<a href="#ref-import.io" role="doc-biblioref">14</a>]</span>. It allows users to add new columns by demonstration but it is not clear whether deleting a column and re-demonstrating it could serve the purpose of repair.</p>
<p>In the prior iteration of Wildcard, if a website’s hand-coded adapter stops working correctly because the website changes, an end-user’s customizations will often break too. Furthermore, end-users cannot extend the scraping adapter to add columns to the table in order to perform new customizations. This goes against MacLean et. al.’s vision of user-tailorable systems <span class="citation" data-cites="maclean1990">[<a href="#ref-maclean1990" role="doc-biblioref">22</a>]</span> that give users “a feeling of ownership of the system, to feel in control of changing the system and to understand what can be changed.” Providing an easy way for users to edit programs is therefore fundamental to fully democratizing web customization.</p>
<p>Editing by demonstration makes end-users first-class citizens in the customization ecosystem. Because users interact with the scraped data through a unified environment directly in the context of the website, it is easy to initiate the scraping system in editing mode: the scraping system is simply booted up using metadata stored with the scraping adapter to the state when the demonstration was completed. Users that have gone through the creation process will immediately realize what to do in order to extend or repair the adapter. Users that have not gone through the creation process might have a harder time but we provide visual clues (such as highlighting the row to perform demonstrations from with a green border) and live programming (immediately preview the results of demonstrations) that serve as guides.</p>
<p>As discussed in Section 3, editing by demonstration in the web scraping domain is feasible because column selectors are independent of each other. However, this is not the case with row selectors because column selectors are dependent on them. Our system therefore does not support editing rows but this an acceptable limitation given our focus on extension and repair which only involve column selectors.</p>
<h2 data-number="4.3" id="live-programming-1"><span class="header-section-number">4.3</span> Live Programming</h2>
<p>In many programming-by-demonstration web scraping systems <span class="citation" data-cites="chasins2018 lin2009">[<a href="#ref-chasins2018" role="doc-biblioref">9</a>,<a href="#ref-lin2009" role="doc-biblioref">19</a>]</span>, users only get <em>full</em> feedback about the program’s execution (result of wrapper induction and the scraped values) after providing <em>all</em> the demonstrations. This means they cannot adjust their demonstrations in response to the system’s feedback as they demonstrate.</p>
<p>Our end-user web scraping system provides level 3 liveness under Tanimoto’s liveness hierarchy <span class="citation" data-cites="tanimoto1990">[<a href="#ref-tanimoto1990" role="doc-biblioref">26</a>]</span>. Level 3 liveness describes a system that is constantly listening for actions from the user, automatically re-constructing and re-executing the program whenever one happens instead of requiring an explicit command from the user. To eliminate the described edit-compile-debug cycle, our system automatically runs the wrapper induction algorithm and generates an adapter after each user demonstration. As we showed in Section 2, when a user demonstrates a value of a column they wish to scrape, our system immediately shows how it has generalized the user’s demonstration across the other rows of the data by highlighting the all relevant values on the website. It also populates the table with the scraped data based on the latest demonstration. The highlighting and table population serve to provide a visual representation of the adapter’s execution.</p>
<p>Many successful end-user programming systems such as spreadsheets and SQL provide users with immediate results after entering commands. Our live programming environment is particularly similar to that of FlashProg <span class="citation" data-cites="mayer2015">[<a href="#ref-mayer2015" role="doc-biblioref">23</a>]</span>, a framework that provides user interface support for programming-by-demonstration systems like FlashExtract <span class="citation" data-cites="le2014">[<a href="#ref-le2014" role="doc-biblioref">17</a>]</span>, and relates to the idea that an important quality of end-user programming is “interaction with a living system” <span class="citation" data-cites="zotero-60">[<a href="#ref-zotero-60" role="doc-biblioref">4</a>]</span>.</p>
<p>Unlike text-based commands which are only valid once complete (e.g <code>SELECT * FRO</code> versus <code>SELECT * FROM user_table</code>), the target of demonstration commands (the value of a DOM element under the cursor) is the same during both hover and click. This allows us to execute a command before a user completes it, thereby providing them with a preview of the results on hover.</p>
<p>There are limits to this approach. Providing live feedback on websites with a large number of DOM elements or complex CSS selectors can slow down the generalization process, especially if a user is constantly moving their cursor. Furthermore, many datasets are too large to preview in the table in their entirety; the user might benefit more from the live feedback if it could summarize large datasets. For example, FlashProg provides a summary of the generalization through a color-coded minimap next to the scrollbar of its extraction interface.</p>
<h1 data-number="5" id="sec:related-work"><span class="header-section-number">5</span> Related Work</h1>
<p>End-user web scraping for customization relates to existing work in end-user web scraping and end-user web customization by a number of tools.</p>
<h2 data-number="5.1" id="end-user-web-scraping"><span class="header-section-number">5.1</span> End-user Web Scraping</h2>
<p>FlashProg <span class="citation" data-cites="mayer2015">[<a href="#ref-mayer2015" role="doc-biblioref">23</a>]</span> is a framework that provides user interface support for FlashExtract <span class="citation" data-cites="le2014">[<a href="#ref-le2014" role="doc-biblioref">17</a>]</span>, a framework for data scraping by examples. FlashProg’s interface provides immediate visual feedback about the generalization and scrapes the matched values into an output tab. In addition, it has a program viewer tab that contains a high level description of what the generated program is doing and provides a list of alternative programs. Finally, it has a disambiguation tab that utilizes conversational clarification to disambiguate programs, the conservations with the user serving as inputs to generate better programs. Though FlashProg has many desirable features we aim to implement in future iterations, its implementation does not align with our goal to provide a unified environment within a browser for scraping and customizing websites.</p>
<p>Rousillon <span class="citation" data-cites="chasins2018">[<a href="#ref-chasins2018" role="doc-biblioref">9</a>]</span> is a tool that enables end-users to scrape distributed, hierarchical web data. Because demonstrations can span across several websites and involve complex data access automation tasks, its interface does not provide <em>full</em> live feedback about its generalizations or the values to be scraped until all the demonstrations have been provided and the generated program has been run. If run on a website it has encountered before, Rousillon makes all the previously determined generalizations visible to the user by color-coding the values on the website that belong to the same column. This is a desirable feature for our system as users will not have to actively explore in order to discover which values are available for scraping and how they are related to each other. On the extension and repair front, Rousillon presents the web scraping code generated by demonstration as an editable, high-level, block-based language called Helena <span class="citation" data-cites="zotero-51">[<a href="#ref-zotero-51" role="doc-biblioref">3</a>]</span>. While Helena can be used to perform more complex editing tasks like adding control flow, it does not support adding or repairing columns after the demonstrations and presents a change in the model used for creation. Our system maintains the model used for creation by allowing users to extend and repair web scraping code via demonstration.</p>
<h2 data-number="5.2" id="end-user-web-customization"><span class="header-section-number">5.2</span> End-user Web Customization</h2>
<p>Vegemite <span class="citation" data-cites="lin2009">[<a href="#ref-lin2009" role="doc-biblioref">19</a>]</span> is a tool for end-user programming of mashups. It has two interfaces: one for scraping values from a website and another for creating scripts that operate on the scraped values. The web scraping interface does not provide live feedback about the generalization on hover but after a user clicks a value, the interface shows the result of the system’s generalization by highlighting the all matched values. Furthermore, even though the interface also has a table, the table is only populated with the scraped values after all the demonstrations have been provided. The scripting interface utilizes CoScripter <span class="citation" data-cites="leshed2008">[<a href="#ref-leshed2008" role="doc-biblioref">18</a>]</span> which is used to record operations on the scraped values for automation. For example, the scripting interface can be used to demonstrate the task of copying an address in the data table, pasting it into a walk score calculator and pasting the result back into the table. The script would then be generalized to all the rows and re-run to fill in the remaining walk scores. CoScripter provides the generated automation program as text-based commands, such as “paste address into ‘Walk Score’ input,” which can be edited after the program is created via “sloppy programming” <span class="citation" data-cites="lin2009">[<a href="#ref-lin2009" role="doc-biblioref">19</a>]</span> techniques. However, this editing does not extend to the web scraping interface used for demonstrations and presents a change in the model used for creation.</p>
<p>Sifter <span class="citation" data-cites="huynh2006">[<a href="#ref-huynh2006" role="doc-biblioref">13</a>]</span> is a tool that augments websites with advanced sorting and filtering functionality. Similarly to Wildcard, it uses web scraping techniques to extract data from websites in order to enable customizations. However, Wildcard supports a broader range of customizations beyond sorting and filtering, including adding annotations to websites and running computations with a spreadsheet formula language. Our scraping system intentionally provides less automation than Sifter. Sifter attempts to automatically detect items and fields on the page with a variety of clever heuristics, including automatically detecting link tags and considering the size of elements on the page. It then gives the user the option of correcting the result if the heuristics do not work properly. In contrast, our heuristics are simpler and make fewer assumptions about the structure of websites. Rather, we give more control to the user from the beginning of the process, and incorporate live feedback to help the user provide useful demonstrations. We hypothesize that focusing on a tight feedback loop rather than automation may support a scraping process that is just as fast as an automated one, but gives the user finer control and extends to a greater variety of websites where more complex heuristics do not apply. However, further user testing is required to actually validate this hypothesis.</p>
<h1 data-number="6" id="sec:conclusion"><span class="header-section-number">6</span> Conclusion And Future Work</h1>
<p>In this paper, we presented our progress towards <em>end-user web scraping for customization</em>, to empower end-users in Wildcard’s ecosystem to create, extend and repair scraping adapters. There are several outstanding issues and open questions we hope to address in future work.</p>
<p>Like existing programming-by-demonstration approaches, web scraping in our current implementation is limited to what can be demonstrated by point-and-click. More generally, this surfaces a fundamental limitation of programming-by-demonstration: the inability to specify logic. One solution for this is taking advantage of spreadsheet formulas which have enabled millions of end-users to specify logic. Because of this, Wildcard already includes a spreadsheet-like formula language for specifying customizations. We plan to extend this language to augment our demonstration-based web scraping with the goal of raising the ceiling on the expressiveness available to end-users. One concrete use case for this is scraping DOM element attributes which cannot be demonstrated by point-and-click but contain valuable data. For example, link elements have an <em>href</em> attribute which contain the URLs associated with the link element and video elements have <em>currentTime</em> and <em>duration</em> attributes which contain data about the current playback position and duration respectively. This use of spreadsheet-like formulas to enable end-users to specify logic is related to approaches taken by Microsoft Power Apps <span class="citation" data-cites="zotero-94">[<a href="#ref-zotero-94" role="doc-biblioref">7</a>]</span>, Glide <span class="citation" data-cites="zotero-108">[<a href="#ref-zotero-108" role="doc-biblioref">1</a>]</span>, Coda <span class="citation" data-cites="zotero-110">[<a href="#ref-zotero-110" role="doc-biblioref">2</a>]</span> and Gneiss <span class="citation" data-cites="chang2014">[<a href="#ref-chang2014" role="doc-biblioref">8</a>]</span>.</p>
<p>To assess our design principles, we plan to carry out a broader evaluation of our system through a user study. So far, we have only tested the system amongst ourselves and a small number of colleagues. More testing is needed to understand whether it can be successfully used among a broader set of users across a wider variety of websites. We also plan to provide more insight, feedback and control into the wrapper induction process. This is particularly important when the system produces an error or is not able to generate an adapter for a website. One solution for this would be to incorporate a version of the program viewer and disambiguation features of FlashProg <span class="citation" data-cites="mayer2015">[<a href="#ref-mayer2015" role="doc-biblioref">23</a>]</span>. Wrangler <span class="citation" data-cites="kandel2011">[<a href="#ref-kandel2011" role="doc-biblioref">15</a>]</span>, whose interface is also centered around a table representation, could also provide some inspiration.</p>
<p>Our ultimate goal is to empower end-users to customize websites in the course of their daily use in an intuitive and flexible way, and thus make the web more malleable for all of its users.</p>
<section id="acknowledgments" class="html-only" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Acknowledgments</h1>
<p>Thank you to Gloria Lin, Joshua Pollock, the members of MIT’s Software Design Group, the anonymous reviewers of PX/21 and the particpants of PX/21 for providing valuable feedback on this work. The authors gratefully acknowledge the support of the CNS division of the National Science Foundation through award number 1801399.</p>
</section>
<div class="pdf-only">

</div>
<h1 class="unnumbered" id="bibliography">References</h1>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-zotero-108" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">Build an app from a <span>Google Sheet</span> in five minutes, for free • <span>Glide</span>. Retrieved March 14, 2021 from <a href="https://www.glideapps.com/">https://www.glideapps.com/</a></div>
</div>
<div id="ref-zotero-110" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">Coda | <span>A</span> new doc for teams. Retrieved March 14, 2021 from <a href="https://coda.io/welcome">https://coda.io/welcome</a></div>
</div>
<div id="ref-zotero-51" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">Helena | <span>Web Automation</span> for <span>End Users</span>. Retrieved February 12, 2021 from <a href="http://helena-lang.org/">http://helena-lang.org/</a></div>
</div>
<div id="ref-zotero-60" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">End-user programming. Retrieved February 12, 2021 from <a href="https://www.inkandswitch.com/end-user-programming.html">https://www.inkandswitch.com/end-user-programming.html</a></div>
</div>
<div id="ref-zotero-90" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">Greasemonkey - <span>GreaseSpot Wiki</span>. Retrieved March 13, 2021 from <a href="https://wiki.greasespot.net/Greasemonkey">https://wiki.greasespot.net/Greasemonkey</a></div>
</div>
<div id="ref-zotero-92" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">Tampermonkey for <span>Chrome</span>. Retrieved March 13, 2021 from <a href="http://www.tampermonkey.net">http://www.tampermonkey.net</a></div>
</div>
<div id="ref-zotero-94" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">Business <span>Apps</span> | <span>Microsoft Power Apps</span>. Retrieved March 13, 2021 from <a href="https://powerapps.microsoft.com/en-us/">https://powerapps.microsoft.com/en-us/</a></div>
</div>
<div id="ref-chang2014" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">Kerry Shih-Ping Chang and Brad A. Myers. 2014. Creating interactive web data applications with spreadsheets. In <em>Proceedings of the 27th annual <span>ACM</span> symposium on <span>User</span> interface software and technology</em>, <span>ACM</span>, <span>Honolulu Hawaii USA</span>, 87–96. DOI:https://doi.org/<a href="https://doi.org/10.1145/2642918.2647371">10.1145/2642918.2647371</a></div>
</div>
<div id="ref-chasins2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">Sarah E. Chasins, Maria Mueller, and Rastislav Bodik. 2018. Rousillon: <span>Scraping Distributed Hierarchical Web Data</span>. In <em>The 31st <span>Annual ACM Symposium</span> on <span>User Interface Software</span> and <span>Technology</span> - <span>UIST</span> ’18</em>, <span>ACM Press</span>, <span>Berlin, Germany</span>, 963–975. DOI:https://doi.org/<a href="https://doi.org/10.1145/3242587.3242661">10.1145/3242587.3242661</a></div>
</div>
<div id="ref-dexi.io" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">dexi.io. The most powerful web scraping software available. Retrieved February 14, 2021 from <a href="https://webscraping.dexi.io">https://webscraping.dexi.io</a></div>
</div>
<div id="ref-drosos2020" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">Ian Drosos, Titus Barik, Philip J. Guo, Robert DeLine, and Sumit Gulwani. 2020. Wrex: <span>A Unified Programming</span>-by-<span>Example Interaction</span> for <span>Synthesizing Readable Code</span> for <span>Data Scientists</span>. In <em>Proceedings of the 2020 <span>CHI Conference</span> on <span>Human Factors</span> in <span>Computing Systems</span></em>, <span>ACM</span>, <span>Honolulu HI USA</span>, 1–12. DOI:https://doi.org/<a href="https://doi.org/10.1145/3313831.3376442">10.1145/3313831.3376442</a></div>
</div>
<div id="ref-hogue2005" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">Andrew Hogue and David Karger. 2005. Thresher: Automating the unwrapping of semantic content from the <span>World Wide Web</span>. In <em>Proceedings of the 14th international conference on <span>World Wide Web</span> - <span>WWW</span> ’05</em>, <span>ACM Press</span>, <span>Chiba, Japan</span>, 86. DOI:https://doi.org/<a href="https://doi.org/10.1145/1060745.1060762">10.1145/1060745.1060762</a></div>
</div>
<div id="ref-huynh2006" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">David F. Huynh, Robert C. Miller, and David R. Karger. 2006. Enabling web browsers to augment web sites’ filtering and sorting functionalities. In <em>Proceedings of the 19th annual <span>ACM</span> symposium on <span>User</span> interface software and technology - <span>UIST</span> ’06</em>, <span>ACM Press</span>, <span>Montreux, Switzerland</span>, 125. DOI:https://doi.org/<a href="https://doi.org/10.1145/1166253.1166274">10.1145/1166253.1166274</a></div>
</div>
<div id="ref-import.io" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">import.io. Data <span>Scraping</span> | <span>Web Scraping</span> | <span>Screen Scraping</span> | <span>Extract</span>. Retrieved February 14, 2021 from <a href="https://www.import.io/product/extract/">https://www.import.io/product/extract/</a></div>
</div>
<div id="ref-kandel2011" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">Sean Kandel, Andreas Paepcke, Joseph Hellerstein, and Jeffrey Heer. 2011. Wrangler: Interactive visual specification of data transformation scripts. In <em>Proceedings of the 2011 annual conference on <span>Human</span> factors in computing systems - <span>CHI</span> ’11</em>, <span>ACM Press</span>, <span>Vancouver, BC, Canada</span>, 3363. DOI:https://doi.org/<a href="https://doi.org/10.1145/1978942.1979444">10.1145/1978942.1979444</a></div>
</div>
<div id="ref-kushmerick2000" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">Nicholas Kushmerick. 2000. Wrapper induction: <span>Efficiency</span> and expressiveness. <em>Artificial Intelligence</em> 118, 1 (April 2000), 15–68. DOI:https://doi.org/<a href="https://doi.org/10.1016/S0004-3702(99)00100-9">10.1016/S0004-3702(99)00100-9</a></div>
</div>
<div id="ref-le2014" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">Vu Le and Sumit Gulwani. 2014. <span>FlashExtract</span>: A framework for data extraction by examples. In <em>Proceedings of the 35th <span>ACM SIGPLAN Conference</span> on <span>Programming Language Design</span> and <span>Implementation</span></em>, <span>ACM</span>, <span>Edinburgh United Kingdom</span>, 542–553. DOI:https://doi.org/<a href="https://doi.org/10.1145/2594291.2594333">10.1145/2594291.2594333</a></div>
</div>
<div id="ref-leshed2008" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">Gilly Leshed, Eben M. Haber, Tara Matthews, and Tessa Lau. 2008. <span>CoScripter</span>: Automating &amp;amp; sharing how-to knowledge in the enterprise. In <em>Proceedings of the <span>SIGCHI Conference</span> on <span>Human Factors</span> in <span>Computing Systems</span></em> (<span>CHI</span> ’08), <span>Association for Computing Machinery</span>, <span>New York, NY, USA</span>, 1719–1728. DOI:https://doi.org/<a href="https://doi.org/10.1145/1357054.1357323">10.1145/1357054.1357323</a></div>
</div>
<div id="ref-lin2009" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">James Lin, Jeffrey Wong, Jeffrey Nichols, Allen Cypher, and Tessa A. Lau. 2009. End-user programming of mashups with vegemite. In <em>Proceedings of the 14th international conference on <span>Intelligent</span> user interfaces</em> (<span>IUI</span> ’09), <span>Association for Computing Machinery</span>, <span>New York, NY, USA</span>, 97–106. DOI:https://doi.org/<a href="https://doi.org/10.1145/1502650.1502667">10.1145/1502650.1502667</a></div>
</div>
<div id="ref-litt2020a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">Geoffrey Litt and Daniel Jackson. 2020. Wildcard: <span>Spreadsheet</span>-<span>Driven Customization</span> of <span>Web Applications</span>. In <em>Companion <span>Proceedings</span> of the 4th <span>International Conference</span> on the <span>Art</span>, <span>Science</span>, and <span>Engineering</span> of <span>Programming</span></em>, <span>Association for Computing Machinery</span>, <span>Porto, Portugal.</span>, 10. DOI:https://doi.org/<a href="https://doi.org/10.1145/3397537.3397541">10.1145/3397537.3397541</a></div>
</div>
<div id="ref-litt2020b" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">Geoffrey Litt, Daniel Jackson, Tyler Millis, and Jessica Quaye. 2020. End-user software customization by direct manipulation of tabular data. In <em>Proceedings of the 2020 <span>ACM SIGPLAN International Symposium</span> on <span>New Ideas</span>, <span>New Paradigms</span>, and <span>Reflections</span> on <span>Programming</span> and <span>Software</span></em>, <span>ACM</span>, <span>Virtual USA</span>, 18–33. DOI:https://doi.org/<a href="https://doi.org/10.1145/3426428.3426914">10.1145/3426428.3426914</a></div>
</div>
<div id="ref-maclean1990" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">Allan MacLean, Kathleen Carter, Lennart Lövstrand, and Thomas Moran. 1990. User-tailorable systems: Pressing the issues with buttons. In <em>Proceedings of the <span>SIGCHI Conference</span> on <span>Human Factors</span> in <span>Computing Systems</span></em> (<span>CHI</span> ’90), <span>Association for Computing Machinery</span>, <span>New York, NY, USA</span>, 175–182. DOI:https://doi.org/<a href="https://doi.org/10.1145/97243.97271">10.1145/97243.97271</a></div>
</div>
<div id="ref-mayer2015" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">Mikaël Mayer, Gustavo Soares, Maxim Grechkin, Vu Le, Mark Marron, Oleksandr Polozov, Rishabh Singh, Benjamin Zorn, and Sumit Gulwani. 2015. User <span>Interaction Models</span> for <span>Disambiguation</span> in <span>Programming</span> by <span>Example</span>. In <em>Proceedings of the 28th <span>Annual ACM Symposium</span> on <span>User Interface Software</span> &amp; <span>Technology</span></em>, <span>ACM</span>, <span>Charlotte NC USA</span>, 291–301. DOI:https://doi.org/<a href="https://doi.org/10.1145/2807442.2807459">10.1145/2807442.2807459</a></div>
</div>
<div id="ref-octoparse" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">Octoparse. Web <span>Scraping Tool</span> &amp; <span>Free Web Crawlers</span> | <span>Octoparse</span>. Retrieved February 14, 2021 from <a href="https://www.octoparse.com/#">https://www.octoparse.com/#</a></div>
</div>
<div id="ref-parsehub" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">ParseHub. <span>ParseHub</span> | <span>Free</span> web scraping - <span>The</span> most powerful web scraper. Retrieved February 14, 2021 from <a href="https://www.parsehub.com/">https://www.parsehub.com/</a></div>
</div>
<div id="ref-tanimoto1990" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">Steven L. Tanimoto. 1990. <span>VIVA</span>: <span>A</span> visual language for image processing. <em>Journal of Visual Languages &amp; Computing</em> 1, 2 (June 1990), 127–139. DOI:https://doi.org/<a href="https://doi.org/10.1016/S1045-926X(05)80012-6">10.1016/S1045-926X(05)80012-6</a></div>
</div>
</div>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-38867184-1', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>
